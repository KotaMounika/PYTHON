{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Answer: Ensemble Learning is a machine learning technique, where multiple models are trained, and combined to solve the same problem, instead of relying on a single model. The final prediction is made by aggregating the predictions of all the models.\n",
        "\n",
        "Key Idea => A group of weak or moderately accurate models can combine together to form a strong model. Each model makes different errors, when the models are combined , these errors tend to cancel out. This leads to better accuracy. Common ensemble techniques are bagging , boosting and stacking."
      ],
      "metadata": {
        "id": "2evmD7O8M2fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bagging - trains multiple independent models on different random samples of dataset and combines their predictions.\n",
        "Models are trained in parallel, each model has equal importance, focuses on reducing variance, works best with high-variance models.\n",
        "\n",
        "Boosting - trains models sequentially, where each new model focuses more on the errors made by previous models.\n",
        "Here models train one after another. Misclassified points get higher weight, focuses on reducing bias, can convert weak learners into strong learners."
      ],
      "metadata": {
        "id": "EMfcaop8NBHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bootstrap sampling is a statistical resampling technique where multiple new datasets are created from the original dataset by sampling with replacement.\n",
        "\n",
        "Each boostrap sample has the same size as the original dataset. Some datapoints may appear multiple times, some datasets may not appear at all.\n",
        "\n",
        "Role of Bootstrap sampling in Bagging & Random Forest.\n",
        "\n",
        "Multiple bootstrap samples are generated from the original dataset, a decision tree is trained on each bootstrap sample. Trees are trained independently and in parallel. Final prediction is made by classification/regression."
      ],
      "metadata": {
        "id": "1hHHeDnQNHcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In bagging methods like Random forest, each model is trained on a bootstrap sample.\n",
        "\n",
        "OOB samples are the datapoints from the original dataset that are not selected in a particular bootstrap sample.\n",
        "\n",
        "On an average about 63% of data points appear in a bootstrap sample.\n",
        "\n",
        "To evaluate ensemble models ---> Each tree is trained on its bootstrap sample. For every datapoint predictions are collected only from trees where that point was OOB, Final aggregated prediction is compared with the true label."
      ],
      "metadata": {
        "id": "RG5c9itJNRMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "Answer:\n",
        "\n",
        "1)Feature Importance at a Single Decision Tree -\n",
        "Based on the impurity reduction (Gini/ Entropy), a feature is important if it creates large impurity decrease at splits.\n",
        "\n",
        "a. depends heavily on one tree structure, b. highly sensitive to data variations, c.prone to overfitting\n",
        "\n",
        "2)Feature Importance in a Random Forest.\n",
        "Importance is averaged across many trees. Each tree is trained on - different bootstrap samples, random subset of features.\n",
        "\n",
        "a. It is more stable and realiable, less sensitive to noise, reduces bias from individual trees.\n"
      ],
      "metadata": {
        "id": "luv3tw60Nb0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "hEZaBaX-NpaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "feature_names = data.feature_names\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "feature_importance_df = feature_importance_df.sort_values(\n",
        "    by='Importance',\n",
        "    ascending=False\n",
        "\n",
        ")\n",
        "\n",
        "print('Top 5 Most Important Features:')\n",
        "print(feature_importance_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWPlS2dURTis",
        "outputId": "633f4dde-ec19-4a9a-ba14-e5e73d6fad87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "(Include your Python code and output in the code box below.)\n",
        "Answer:"
      ],
      "metadata": {
        "id": "j-kZ9uvFNs7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "bagging = BaggingClassifier(estimator = DecisionTreeClassifier(), n_estimators = 100, random_state = 42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "print(f'Single Decision Tree Accuracy: {accuracy}')\n",
        "print(f'Bagging Classifier Accuracy: {bagging_accuracy}')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7IO0yMsNwrM",
        "outputId": "0d166334-32f0-4445-e333-69c5523546d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "(Include your Python code and output in the code box below."
      ],
      "metadata": {
        "id": "WssJ97i8OQAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20]\n",
        "}\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Test Accuracy:\", final_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrIlqXWvOVWG",
        "outputId": "6444b5fa-4076-4e29-d466-77b653fe4559"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Test Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "5Su1_SKhOe-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset (offline)\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n",
        "\n",
        "\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1ZPp6p4On5S",
        "outputId": "d32d5d16-a4cd-4cca-9392-25a7776f47d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 2970.863235955056\n",
            "Random Forest Regressor MSE: 2952.0105887640448\n",
            "Bagging Regressor MSE: 2970.863235955056\n",
            "Random Forest Regressor MSE: 2952.0105887640448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Choosing between Bagging and Boosting - Analyze the data & problem\n",
        "\n",
        "Loan default data is usually:\n",
        "\n",
        "Noisy\n",
        "\n",
        "High-dimensional\n",
        "\n",
        "Imbalanced\n",
        "\n",
        "Decision logic:\n",
        "\n",
        "If the base model overfits → choose Bagging\n",
        "\n",
        "If the base model underfits → choose Boosting\n",
        "\n",
        "Final choice in this case:\n",
        "\n",
        "Start with Bagging (Random Forest) for a stable baseline\n",
        "\n",
        "Move to Boosting (Gradient Boosting / XGBoost) if higher accuracy is needed\n",
        "\n",
        "✔ Reason: Boosting captures complex patterns in customer behavior better.\n",
        "\n",
        "2️⃣ Handling Overfitting\n",
        "\n",
        "Techniques used:\n",
        "\n",
        "Bagging:\n",
        "\n",
        "Bootstrap sampling reduces variance\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Limit max_depth\n",
        "\n",
        "Use learning_rate\n",
        "\n",
        "General strategies:\n",
        "\n",
        "Early stopping\n",
        "\n",
        "Regularization\n",
        "\n",
        "Feature selection\n",
        "\n",
        "Pruning trees\n",
        "\n",
        "✔ Result: Model generalizes better to unseen customers.\n",
        "\n",
        "3️⃣ Selecting Base Models\n",
        "\n",
        "Criteria:\n",
        "\n",
        "Should be simple and interpretable\n",
        "\n",
        "Should have high variance (to benefit from ensembling)\n",
        "\n",
        "Common choices:\n",
        "\n",
        "Decision Trees (depth-controlled)\n",
        "\n",
        "Logistic Regression (baseline comparison)\n",
        "\n",
        "✔ Decision Trees are preferred because:\n",
        "\n",
        "Handle non-linearity\n",
        "\n",
        "Capture feature interactions\n",
        "\n",
        "Work well with ensemble methods.\n",
        "\n",
        "4️⃣ Evaluating Performance Using Cross-Validation\n",
        "\n",
        "Why cross-validation?\n",
        "\n",
        "Prevents biased evaluation\n",
        "\n",
        "Ensures stability across different data splits\n",
        "\n",
        "Approach:\n",
        "\n",
        "Use Stratified K-Fold CV (important for imbalanced loan data)\n",
        "\n",
        "Evaluate using:\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Precision & Recall\n",
        "\n",
        "F1-score\n",
        "\n",
        "✔ Cross-validation ensures the model performs consistently across customer segments.\n",
        "\n",
        "5️⃣ Justifying the Final Ensemble Choice\n",
        "\n",
        "Final justification:\n",
        "\n",
        "Ensemble methods:\n",
        "\n",
        "Reduce bias and variance\n",
        "\n",
        "Handle complex financial patterns\n",
        "\n",
        "Improve predictive power\n",
        "\n",
        "Boosting often performs best for loan default prediction due to its focus on hard-to-classify customers\n",
        "\n",
        "Business justification:\n",
        "\n",
        "Fewer bad loans approved\n",
        "\n",
        "Better risk control\n",
        "\n",
        "Regulatory-friendly explainability (via feature importance)"
      ],
      "metadata": {
        "id": "jPBCNaq9Oz3G"
      }
    }
  ]
}